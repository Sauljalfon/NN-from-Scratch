{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6146578,"sourceType":"datasetVersion","datasetId":3524853}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib as plt\nimport copy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Pre-Processing","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/heart-attack-analysis-prediction-dataset/heart.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ","metadata":{}},{"cell_type":"markdown","source":"# Forward Prop","metadata":{}},{"cell_type":"code","source":"def linear_forward(A, W, b):\n    Z = np.dot(W, A) + b\n    cache = (A, W, b)\n    return Z, cache","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sigmoid(Z):\n    A = 1 / (1 + np.exp(-Z))\n    cache = Z\n    \n    return A, cache","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def relu(Z):\n    A = np.maximum(0, Z)\n    cache = Z\n    \n    return A, cache","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def linear_activation_forward(A_prev, W, b, activation):\n    if activation == \"sigmoid\":\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = sigmoid(Z)\n    elif activation == \"relu\":\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = relu(Z)\n        \n    cache = (linear_cache, activation_cache)\n    \n    return A, cache","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def forward_propagation(X, parameters):\n    caches= []\n    A = X\n    L = len(parameters) // 2\n    for l in range(1, L):\n        A_prev = A\n        A, cache = linear_activation_forward(A_prev, parameters[f\"W{l}\"], parameters[f\"b{l}\"], \"relu\")\n        caches.append(cache)\n    AL, cache = linear_activation_forward(A, parameters[f\"W{L}\"], parameters[f\"b{L}\"], \"sigmoid\")\n    caches.append(cache)\n    \n    return AL, caches\n\ndef initialize_parameters_deep(layer_dims):\n    \n    parameters = {}\n    L = len(layer_dims)\n\n    for l in range(1, L):\n        parameters[f\"W{str(l)}\"] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n        parameters[f\"b{str(l)}\"] = np.zeros([layer_dims[l], 1])\n\n        \n    return parameters\n\nparameters = initialize_parameters_deep([5, 3, 1])\nX = np.random.randn(5,200)\nAL, caches = forward_propagation(X, parameters)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cost Function","metadata":{}},{"cell_type":"code","source":"def compute_cost(AL, Y):\n    m = Y.shape[1]\n    cost = -1/m * np.sum(np.multiply(Y, np.log(AL)) + np.multiply((1 - Y), np.log(1-AL)))\n    cost = np.squeeze(cost)\n    \n    return cost\n\nY = np.random.randn(1,200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Back Prop","metadata":{}},{"cell_type":"code","source":"def linear_backward(dZ, cache):\n    A_prev, W, b = cache\n    m = A_prev.shape[1]\n    \n    dW = 1/m * np.dot(dZ, A_prev.T)\n    db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n    dA_prev = np.dot(W.T, dZ)\n    \n    return dA_prev, dW, db","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def relu_backward(dA, cache):\n    Z = cache\n    dZ = np.array(dA, copy=True)\n    dZ[Z <= 0] = 0\n    assert (dZ.shape == Z.shape)\n\n    return dZ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sigmoid_backward(dA, cache):\n    Z = cache\n    s = 1/(1+np.exp(-Z))\n    dZ = dA * s * (1-s)\n    assert (dZ.shape == Z.shape)\n\n    return dZ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def linear_activation_backward(dA, cache, activation):\n    linear_cache, activation_cache = cache\n    \n    if activation == 'relu':\n        dZ = relu_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n    \n    elif activation == 'sigmoid':\n        dZ = sigmoid_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n        \n    return dA_prev, dW, db","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" def back_propagation(AL, Y, caches):\n        \n    grads = {}\n    L = len(caches)\n    m = AL.shape[1]\n    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n    \n    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n    \n    current_cache = caches[-1]\n    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n    grads[\"dA\" + str(L-1)] = dA_prev_temp\n    grads[\"dW\" + str(L)] = dW_temp\n    grads[\"db\" + str(L)] = db_temp\n    \n    for l in reversed(range(L-1)):\n        current_cache = caches[0]\n        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, \"relu\")\n        grads[\"dA\" + str(l)] = dA_prev_temp\n        grads[\"dW\" + str(l + 1)] = dW_temp\n        grads[\"db\" + str(l + 1)] = db_temp\n\n    return grads\n\ngrads = L_model_backward(AL, Y, caches)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def update_parameters(parameters, grads, learning_rate):\n    parameters = copy.deepcopy(parameters)\n    L = len(parameters) // 2 \n    \n    for l in range(L):\n        parameters[\"W\" + str(l+1)] = parameters['W' + str(l+1)] - learning_rate * grads['dW' + str(l+1)]\n        parameters[\"b\" + str(l+1)] = parameters['b' + str(l+1)] - learning_rate * grads['db' + str(l+1)]\n        \n    return parameters","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model(X, Y, layer_dims, learning_rate = 0.0075, num_iterations = 15000, print_cost = False):\n    costs = []\n    for i in range(num_iterations):\n        # For Prop\n        AL, caches = forward_propagation(X, parameters)\n        # Compute Cost\n        cost = compute_cost(AL, Y)\n        # Back Prop\n        grads = back_propagation(AL, Y, caches)\n        # Update Params\n        parameters = update_parameters(parameters, grads, learning_rate)\n        \n        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n        if i % 100 == 0 or i == num_iterations:\n            costs.append(cost)\n            \n        return parameter, costs","metadata":{"execution":{"iopub.status.busy":"2024-04-25T22:41:39.398921Z","iopub.execute_input":"2024-04-25T22:41:39.399321Z","iopub.status.idle":"2024-04-25T22:41:39.415823Z","shell.execute_reply.started":"2024-04-25T22:41:39.399285Z","shell.execute_reply":"2024-04-25T22:41:39.414736Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}